{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "979c958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cffi\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from enum import Enum\n",
    "from Bio import SeqIO\n",
    "from Bio import Entrez\n",
    "from pathlib import Path\n",
    "from modules.auxiliary import DATA_SEQ_DIR\n",
    "from modules.pytrsomix import TRScalculator, SeqAnalyzer , SeqProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "import glob\n",
    "import re\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cce4977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mulitple blast_output directories found. Please select one:\n",
      "1. /home/hubert/TRS-omix/python/klebsiella_results_L100_R100_c1.0/blast_output\n",
      "2. /home/hubert/TRS-omix/python/citrobacter_results_L50_R50_c1.0/blast_output\n",
      "3. /home/hubert/TRS-omix/python/klebsiella_results_L200_R200_c1.0/blast_output\n",
      "4. /home/hubert/TRS-omix/python/klebsiella_results_L50_R50_c1.0/blast_output\n",
      "5. /home/hubert/Old_results/klebsiella_results_L100_R100_c1.0/blast_output\n",
      "6. /home/hubert/Old_results/klebsiella_results_L200_R200_c0.75/blast_output\n",
      "7. /home/hubert/Old_results/klebsiella_results_L200_R200_c1.0/blast_output\n",
      "8. /home/hubert/Old_results/klebsiella_results_L100_R100_c0.75/blast_output\n",
      "9. /home/hubert/Old_results/klebsiella_results_L50_R50_c1.0/blast_output\n",
      "10. /home/hubert/Old_results/klebsiella_results_L50_R50_c0.75/blast_output\n",
      "Enter the number of the directory you want to use (or exit by typing 'exit'): 2\n",
      "Selected directory: /home/hubert/TRS-omix/python/citrobacter_results_L50_R50_c1.0/blast_output\n",
      "No non-txt files were found.\n",
      "No .txt files found in the directory after attempting to rename. Exiting program.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Exiting program because no .txt files were found.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Exiting program because no .txt files were found.\n"
     ]
    }
   ],
   "source": [
    "#Look for blast_output directory\n",
    "blast_output_path = SeqProcessor.find_directory_by_name('blast_output')\n",
    "\n",
    "\n",
    "if blast_output_path:\n",
    "    print(f\"Selected directory: {blast_output_path}\")\n",
    "else:\n",
    "    # If blast_output_path is either None or an empty string, prompt the user for the path.\n",
    "    print(\"No 'blast_output' directory selected or found.\")\n",
    "    blast_output_path = input(\"Please provide the path to the 'blast_output' directory: \")\n",
    "\n",
    "#Renames files to .txt and find unique sequence ID - accession pairs  \n",
    "SeqProcessor.process_blast_files_in_directory(blast_output_path)\n",
    "\n",
    "blast_output_path= Path(blast_output_path)\n",
    "results_directory = blast_output_path.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Finds the taxonomy database \n",
    "on the users computer name is hardcoded initially as this is the name of the unpacked database from NCBI's FTP\n",
    "'''\n",
    "taxonomy_file = SeqProcessor.find_file_by_name('nucl_gb.accession2taxid',auto=True)\n",
    "if taxonomy_file:\n",
    "    print(f\"Selected file: {taxonomy_file}.\")\n",
    "else:\n",
    "    print(f\"No taxonomy selected or found.\")\n",
    "    taxonomy_file = input(\"Please provide the path to nucl_gb.accession2taxid or equivalent: \")\n",
    "'''\n",
    "Get accessions present in blast files and filter the taxonomy_file using them this way we have all accession - taxid\n",
    "Pairs in out dataset and there is not need to access such a large database.\n",
    "In addition as you can see I'm using chunks to process the initial database, that size could be altered but i found \n",
    "50000 to be sweet spot. Loading progress bar was added for this operation as well.\n",
    "'''\n",
    "accessions = SeqProcessor.collect_accessions_from_blast_files(blast_output_path)\n",
    "tax_df = SeqProcessor.filter_taxonomy_file(taxonomy_file,accessions,50000)\n",
    "\n",
    "\n",
    "'''\n",
    "Saving the filtered database to a csv file for use later - NOT IMPLEMENTED\n",
    "'''\n",
    "output_file = os.path.join(blast_output_path, \"taxonomy_filtered.csv\")\n",
    "tax_df.to_csv(output_file, index=False)\n",
    "print(f\"tax_df saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Quite self-explanatory makes sure that each taxid - accession pair gets put into dictionary \n",
    "only once i make use of sets here to decrease it's size and repeated information\n",
    "'''\n",
    "taxid_accessions_dict = {}\n",
    "for index, row in tax_df.iterrows():\n",
    "    accession_column = tax_df.columns[0]  # Extract accession column dynamically\n",
    "    taxid_column = tax_df.columns[1]  # Extract taxid column dynamically\n",
    "    \n",
    "    accession = row[accession_column]\n",
    "    taxid = row[taxid_column]\n",
    "    \n",
    "    if taxid in taxid_accessions_dict:\n",
    "        taxid_accessions_dict[taxid].append(accession)\n",
    "    else:\n",
    "        taxid_accessions_dict[taxid] = [accession]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the directory for modified files exists\n",
    "modified_blast_path = os.path.join(blast_output_path, \"modified_blast\")\n",
    "SeqProcessor.ensure_directory_exists(modified_blast_path)  # Ensure this function creates the directory if it doesn't exist\n",
    "\n",
    "# Optimized: Invert the taxid_accessions_dict for efficient lookup\n",
    "'''\n",
    "This step assumes each accession maps to exactly one taxid which is true (but that matching is not always accurate)\n",
    "this drawback is attributed to nucleotide database structure and is not something that should alter the results in any\n",
    "significant way as long as we remember to add the top-level species to the dictionary later\n",
    "'''\n",
    "accession_to_taxid = {accession: taxid for taxid, accessions in taxid_accessions_dict.items() for accession in accessions}\n",
    "\n",
    "def map_accession_to_taxid(accession):\n",
    "    \"\"\"Map accession to TaxID using the optimized lookup dictionary.\"\"\"\n",
    "    return accession_to_taxid.get(accession, '')  # Return empty string if accession not found\n",
    "\n",
    "# Iterate over files in the input directory\n",
    "for filename in os.listdir(blast_output_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        input_file_path = os.path.join(blast_output_path, filename)\n",
    "        \n",
    "        # Read the file into a DataFrame\n",
    "        df = pd.read_csv(input_file_path, sep='\\t', header=None)  # No column names specified\n",
    "        \n",
    "        # Map accessions to TaxIDs and add them as a new column after the last existing column\n",
    "        df[df.shape[1]] = df[1].map(map_accession_to_taxid)  # Assuming column 1 contains the accessions\n",
    "        \n",
    "        # Construct output file path\n",
    "        modified_file_path = os.path.join(modified_blast_path, f\"taxids_{filename}\")\n",
    "        \n",
    "        # Save the modified DataFrame to a new file in the output directory\n",
    "        df.to_csv(modified_file_path, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b57d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is still experimental but should allow to load interiors.txt equivalent into a dataframe to construct the\n",
    "dictionary of genomes which we analyze.'''\n",
    "print(\"Searching for the results file....\")\n",
    "results_file = SeqProcessor.find_file_by_name('*_results.csv',folder = results_directory, auto=True)\n",
    "if results_file:\n",
    "    print(f\"Selected file: {results_file}\")\n",
    "else:\n",
    "    # If results_file is either None or an empty string, prompt the user for the path.\n",
    "    print(\"No '*_results.csv' file selected or found.\")\n",
    "    blast_output_path = input(\"Please provide the path to the current experiments result file: \")\n",
    "\n",
    "combined_results = pd.read_csv(results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d62c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set mail for Entrez, extract unique genome ids from dataframe\n",
    "'''\n",
    "SeqProcessor.set_user_email()\n",
    "ncbi_ids = combined_results[\"GENOME\"].unique().tolist()\n",
    "'''\n",
    "Retrieves organism names first, then retrieves taxids for those IDs, creates a new dict name:taxid\n",
    "'''\n",
    "tax_map = SeqProcessor.fetch_organism_taxids(ncbi_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filtering functions gets rid of _parts_ that contain numbers or the capital letter(save for the first one)\n",
    "Should get rid of strains and make it possible to find the taxid of the species in the next step\n",
    "'''\n",
    "filtered_organism_taxid_map = {SeqProcessor.filter_key_parts(key): value for key, value in tax_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_organism_taxid_map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adds additional taxids after clearing out the strain associated info in current klebsiella dataset \n",
    "because of that Klebsiella_pneumoniae_subsp._pneumoniae has two taxids one strain specific and the one associated with subspecies'''\n",
    "species_info = SeqProcessor.append_taxids_to_filtered_map(filtered_organism_taxid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a067bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Additional NCBI IDs can be provided to include their taxids in the dictionary used for filtering down the line\n",
    "Most useful to add top level species in case of klebsiella 573 - klebsiella pneumoniae'''\n",
    "SeqProcessor.interact_and_update_dict(filtered_organism_taxid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create species_info dictionary that contains keys as species/subspecies/strains and values as sets of taxids'''\n",
    "species_info = SeqProcessor.append_taxids_to_filtered_map(filtered_organism_taxid_map)\n",
    "species_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Once again cleaning the names in case we added strains and we dont want to miss valid sequences because of that'''\n",
    "species_info = {SeqProcessor.filter_key_parts(key): value for key, value in species_info.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383957e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Construct a dictionary from analyzed files that includes seqID as key and taxids as sets of values\n",
    "add NaN file for entries that had no valid taxids associated with them it's a sign that either blast database or taxonomy db needs updating'''\n",
    "modified_blast_path = os.path.join(blast_output_path, \"modified_blast\")\n",
    "nan_file = os.path.join(modified_blast_path,\"NaN acessions.csv\")\n",
    "# Construct dictionary of all taxid - acessions pairs in our data\n",
    "results_dict = SeqProcessor.construct_dict_from_files(modified_blast_path,nan_file)\n",
    "'''Most of the time the given sequence is still preserved in the results its presence in the NaN file simply means that one of the acessions got deleted/updated'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We can add exceptions now didn't try it with files though probably \n",
    "should try to introduce the same ability for interact_and_update_dict'''\n",
    "exceptions = SeqProcessor.ask_for_exceptions()\n",
    "print(f\"Exceptions to filtering added: {exceptions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd988b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Convert values to int'''\n",
    "for key, value_set in results_dict.items():\n",
    "    results_dict[key] = {int(val) for val in value_set}    \n",
    "    \n",
    "for key, value_set in species_info.items():\n",
    "    species_info[key] = {int(val) for val in value_set}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c026c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf6e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This filtering step does the following:\n",
    "1. Looks through all key-value set pairs in results_dict\n",
    "2. If one of the values in the set is in exception removes it from the set(but remembers it)\n",
    "3. If leftover values match atleast one present in species_info \n",
    "AND it's the only one associated with a given sequence the record is kept\n",
    "4. That means that the records which had more than one value associated with it but the other ones were exceptions\n",
    "are preserved\n",
    "5.Unfortunetely this didnt help with the Klebsiella_pneumoniae_subsp._pneumoniae completely disappearing from the dataset\n",
    "6. Still this is not a bug but expected behaviour as KP sequences match to A LOT of taxids'''\n",
    "filtered_keys = SeqProcessor.filter_with_exceptions(results_dict,species_info,exceptions)\n",
    "\n",
    "filtered_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Should probably get rid of that but i want to see what happens if we have multi value sets. Did not happen so far.\n",
    "If errors are encountered at this step just comment out the code and include filtered_keys_final = filtered_keys'''\n",
    "filtered_keys_final = SeqProcessor.unpack_single_element_sets(filtered_keys)\n",
    "filtered_keys_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write out to a file a list of rows containing the keys listed in filtered_keys_final'''\n",
    "SeqProcessor.process_files_with_filter(modified_blast_path,filtered_keys_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0208b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Summary function is used here it stores various info about the pipeline - still WIP'''\n",
    "summary_dir = os.path.join(results_directory,\"summary\")\n",
    "SeqProcessor.ensure_directory_exists(summary_dir)\n",
    "summary_path = os.path.join(summary_dir,\"summary.txt\")\n",
    "SeqProcessor.write_summary(summary_path,results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd4898",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Kind of unecessary - there is a simpler way but i have a stable pipeline now and Ill use it to run some experiments'''\n",
    "fasta_processed_path = SeqProcessor.find_file_by_name('unique_taxids_filtered_sequences_combined_unique_blastn_out.txt',auto= True, folder= results_directory)\n",
    "fasta_processed_path = Path(fasta_processed_path)\n",
    "fasta_processed_path = fasta_processed_path.parent\n",
    "fasta_processed_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736fd77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''My naming convention is getting a little bit confusing... if someone wants to fix it go ahead\n",
    "This function finds twins in our files (the ones with the same number at the end) \n",
    "sequences from the same initial sequence keeps them if it find 2'''\n",
    "SeqProcessor.filter_and_overwrite_files_in_directory(fasta_processed_path,file_pattern=\"*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_directory = SeqProcessor.find_directory_by_name_new('unique_sequences', auto = True, folder=results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacaba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ids_dict, special_dict = SeqProcessor.read_sequence_ids(unique_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faadfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Gets sequence of given id from initial .fasta file (still dealing with short sequences) \n",
    "a new function could be written that afterwards strips the numbers from the 'twins' file and uses\n",
    "the *_results.csv dataframe this way the full sequences can be extracted'''\n",
    "output_directory = os.path.join(results_directory,\"final_output\")\n",
    "filtered_fasta_file = SeqProcessor.find_file_by_name('filtered_sequences_combined_unique.fasta', auto= True, folder= results_directory)\n",
    "SeqProcessor.ensure_directory_exists(output_directory)\n",
    "SeqProcessor.filter_fasta_file_dict(filtered_fasta_file,sequence_ids_dict,special_dict,output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fabae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TRS] *",
   "language": "python",
   "name": "conda-env-TRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
